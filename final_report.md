# Learning from networks project documentation

- Luca Crema 2026827
- Riccardo Crociani 2022466

## Title

How hard is it to work on a new raw dataset? Which **useful** metrics can be computed in reasonable time?

## Motivation

We want to explore the process of **data gathering**, **convert** them **to a graph** and **compute meaningful metrics** on it.

## Data

Exchanged Traded Funds (ETFs) components datasets.

An ETF is an asset traded on the stock market just like any other stock that usually tracks a stock index.
The ETF's "underlying" components are the same as the tracked index's component and are bought in the same percentage as defined in the index.
An ETF is emitted and sold by an "Authorized Partecipant" which is in charge of making sure the ETF is following the index as closely as possible.

### Where to find it

Start from an initial dataset found for free [at this link](https://masterdatareports.com/), which is updated at around the end of each month.

### What kind of graph

We want to produce a weighted directed graph. The nodes can represent fund or component, note that a fund node can also be a component. Edges go from a fund node to a component node and the weight is the amount of money in euros the fund owns of that component.

### Number of nodes and edges

We were able to extract 25396 nodes and 396114 edges from the dataset which is a constant factor away from what the sizes we expected.
We have about 1500 ETF nodes and 21000 components, where an ETF has 250 components on average and a component is part of 84 ETFs on average.

## Method


The method we are going to use to parse the gathered data into a graph is mostly based on trial and error but although each dataset has its own types of errors we can apply standard data cleaning techniques.

### Data cleaning


### Data parsing

### Graph analysis

Once the datasets are clean and parsed, we applied some of the node-level metrics, network patterns, network clustering algorithms, and node embeddings, exploring the limits of the exact computations and the reachable levels of approximation.
- #### In-degree(v) and out-degree(v):
    They represent respectively the number of funds buying node v and the number of components bought by fund v.
- #### Longest path:
    Represents the length of the chain of funds buying other funds.
- #### Betweennes centrality (approximated):
    It measures how central is a node in the graph by means of shortest paths. This measure shouldn't convey much information as all the paths are short and the graph is sparsely connected.
    Since the graph is too large to compute the exact betweenness centrality of each node, we use `ex.betweenness_centrality_percent(G, percentage=p)` to compute an approximation on a small percentage of the nodes such that the computation takes reasonable time (few minutes).
- #### Closeness centrality:
    It measures how close is a node to all the other nodes in the graph.
    Although this metric shouldn't convey intresting information, like betweenness centrality, we computed it twice. The first is an exact method using Floyd-Warshall algorithm with adjacency matrix which is much more efficient then standard networkx's and runs in reasonable time.
    The second is also an exact algorithm from networkx, but it runs on a randomly connected subgraph generated by the method `ex.connected_random_subgraph(G: nx.Graph, n: int)` we devised.
- #### Clustering coefficient:
    Although the graph is very large the clustering coefficient can be computed exactly because a triangle is a very rare occurrency.
    In this case a triangle represent a fund bought by another fund, and both buying the same component.
    Computation is done using the networkx algorithm `nx.clustering(G, weight="weight")`.

- #### Embedding using node2vec approach
    A quick way to get an idea of the graph structure are embeddings.
    We used the python implementation of node2vec to embed the graph in two dimension and visulize it through matplotlib.
- #### ESU algorithm:
    Since the graph is very large the computation of all the subgraphs of size k, for k=1,2,3,4 takes too long.
    In our case the enumeration of all the graphlets doesn't convey relevant information because we already have an idea of what are the frequencies of small patterns in the graph.

## Results

The answer to the question "is it hard to clean, parse and analyze new datasets?" is of course yes, it is. Most of the difficulty comes from the unexpected errors that can be found in a dirty dataset, which are not predictable or cannot always be spotted.
The standard cleaning methods we adopted were only partially helpful, as they have to be adapted to the specific problem and do not always improve the data quality. It was very difficult to balance filtering (removing wrong rows) with fixing (interpolation or copying missing attributes from other rows), the former leading to a greater loss of data and the latter leading to errors and inconsistencies.
Another critical part of the project was finding, devising and adapting existing algorithms because most of what we found in libraries were exact algorithms with infeasible computation times for our graph.

## References

- [Master data reports initial dataset](https://masterdatareports.com/)
- [Networkx library](https://networkx.org/)
- [Pandas library](https://pandas.pydata.org/)
- [Numpy library](https://numpy.org/)
- [Scipy library](https://scipy.org/)
- [Official python wiki: Graph computation resources](https://wiki.python.org/moin/PythonGraphApi)
- [Igraph library](https://igraph.org)
- [Node2vec implementation](https://github.com/eliorc/node2vec)
- [Closeness centrality Floyd-Warshall implementation](https://medium.com/@pasdan/closeness-centrality-via-networkx-is-taking-too-long-1a58e648f5ce)
- [ESU algorithm implementation](https://notebook.community/ramseylab/networkscompbio/class18_motifs_python3_template)
